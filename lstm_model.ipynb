{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AnacondaIDE\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from scipy import stats\n",
    "from pylab import rcParams\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import folium\n",
    "import fbprophet\n",
    "plt.style.use('fivethirtyeight')\n",
    "#import pickle\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AnacondaIDE\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import itertools\n",
    "from datetime import timedelta\n",
    "\n",
    "#from joblib import Parallel, delayed\n",
    "#import multiprocessing\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.models import load_model\n",
    "#from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_16 = pd.read_csv('D:/ML work/Zillow Home Value Prediction/train_2016_v2/train_2016_v2.csv')\n",
    "properties_16 = pd.read_csv('D:/ML work/Zillow Home Value Prediction/properties_2016/properties_2016.csv')\n",
    "\n",
    "train_17 = pd.read_csv('D:/ML work/Zillow Home Value Prediction/train_2017.csv/train_2017.csv')\n",
    "properties_17 = pd.read_csv('D:/ML work/Zillow Home Value Prediction/properties_2017.csv/properties_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_16['transactiondate'] = pd.to_datetime(train_16['transactiondate'], format = '%Y-%m-%d')\n",
    "train_17['transactiondate'] = pd.to_datetime(train_17['transactiondate'], format = '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "properties_df_16 = properties_16.merge(train_16, how = 'left', on = 'parcelid')\n",
    "del properties_16, train_16\n",
    "\n",
    "properties_df_17 = properties_17.merge(train_17, how = 'left', on = 'parcelid')\n",
    "del properties_17, train_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "properties_df = properties_df_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del properties_df_16#, properties_df_17\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "properties_df.drop(['taxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxamount'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for c, dtype in zip(properties_df.columns, properties_df.dtypes):\n",
    "    if dtype == np.float64:\n",
    "        properties_df[c] = properties_df[c].astype(np.float32) \n",
    "    elif dtype == np.int64:\n",
    "        properties_df[c] = properties_df[c].astype(np.int32)\n",
    "#end for loop\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01145784743130207, 0.0060000001, 0.16107094287872314)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_mean = np.mean(properties_df['logerror'])\n",
    "error_median = np.median(properties_df['logerror'].dropna())\n",
    "error_std = np.std(properties_df['logerror'])\n",
    "\n",
    "error_mean, error_median, error_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "properties_df['calculatedbathnbr'] = np.where(np.isnan(properties_df['calculatedbathnbr']), properties_df['bathroomcnt'], properties_df['calculatedbathnbr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop bathroomcnt\n",
    "properties_df.drop('bathroomcnt', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So we can safely drop one column of them\n",
    "properties_df.drop('finishedsquarefeet50', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if condition is satisfied then replace with True else False\n",
    "# assume that if fireplacecnt is null, then fireplaceflg will be false\n",
    "properties_df['fireplaceflag'] = np.where(pd.isnull(properties_df['fireplaceflag']) & pd.notnull(properties_df['fireplacecnt']) & properties_df['fireplacecnt'] > 0, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2985342"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties_df['fireplaceflag'].count()\n",
    "# fireplaceflag is recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = properties_df['fireplacecnt'].isnull()\n",
    "properties_df.loc[index, 'fireplacecnt'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "properties_df['house_age'] = datetime.now().date().year - properties_df['yearbuilt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#assume if taxdelinquencyflag is null then False (doesn't exist)\n",
    "index = properties_df['taxdelinquencyflag'].isnull()\n",
    "properties_df.loc[index, 'taxdelinquencyflag'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "properties_df.drop('pooltypeid10', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this does not make sense in theory. it should be assigned poolcnt - pooltypeid7\n",
    "index = (properties_df['pooltypeid2'].isnull()) & (properties_df['hashottuborspa'] == True) & (properties_df['poolcnt'].notnull()) & ((properties_df['pooltypeid7']).notnull())\n",
    "properties_df.loc[index, 'pooltypeid2'] = properties_df.loc[index, 'poolcnt'] - properties_df.loc[index, 'pooltypeid7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = (properties_df['pooltypeid2'].isnull()) & (properties_df['hashottuborspa'] == True) & (properties_df['poolcnt'].notnull()) & ((properties_df['pooltypeid7']).isnull())\n",
    "properties_df.loc[index, 'pooltypeid2'] = properties_df['poolcnt']\n",
    "\n",
    "index = (properties_df['pooltypeid2'].isnull()) & (properties_df['poolcnt'].isnull())\n",
    "properties_df.loc[index, 'pooltypeid2'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this does not make sense, assign poolcnt to pooltypeid7\n",
    "index = (properties_df['pooltypeid7'].isnull()) & (properties_df['poolcnt'].notnull() & (properties_df['pooltypeid2'].notnull())) \n",
    "properties_df.loc[index, 'pooltypeid7'] = properties_df.loc[index, 'poolcnt'] - properties_df.loc[index, 'pooltypeid2']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = properties_df.hashottuborspa.isnull()\n",
    "properties_df.loc[index, 'hashottuborspa'] = False\n",
    "\n",
    "index = properties_df.pooltypeid2.isnull()\n",
    "properties_df.loc[index,'pooltypeid2'] = 0\n",
    "\n",
    "index = properties_df.pooltypeid7.isnull()\n",
    "properties_df.loc[index,'pooltypeid7'] = 0\n",
    "\n",
    "index = properties_df.poolcnt.isnull()\n",
    "properties_df.loc[index,'poolcnt'] = 0\n",
    "\n",
    "index = (properties_df.poolcnt == 0)\n",
    "properties_df.loc[index, 'poolsizesum'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace 'Y' with True in taxdelinquencyflag\n",
    "index = (properties_df.taxdelinquencyflag == 'Y')\n",
    "properties_df.loc[index,'taxdelinquencyflag'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "properties_df['taxdelinquencyflag'] = properties_df.taxdelinquencyflag.astype(np.bool)\n",
    "properties_df['hashottuborspa'] = properties_df.hashottuborspa.astype(np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Null in garage count means there are no garages\n",
    "index = properties_df.garagecarcnt.isnull()\n",
    "properties_df.loc[index,'garagecarcnt'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#error in calculation of the finished living area of home\n",
    "properties_df['N-LivingAreaError'] = properties_df['calculatedfinishedsquarefeet'] / properties_df['finishedsquarefeet12']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#proportion of living area\n",
    "properties_df['N-LivingAreaProp'] = properties_df['calculatedfinishedsquarefeet'] / properties_df['lotsizesquarefeet']\n",
    "properties_df['N-LivingAreaProp2'] = properties_df['finishedsquarefeet12'] / properties_df['finishedsquarefeet15']\n",
    "\n",
    "#Amout of extra space\n",
    "properties_df['N-ExtraSpace'] = properties_df['lotsizesquarefeet'] - properties_df['calculatedfinishedsquarefeet'] \n",
    "properties_df['N-ExtraSpace-2'] = properties_df['finishedsquarefeet15'] - properties_df['finishedsquarefeet12']\n",
    "\n",
    "#Average room size\n",
    "properties_df['N-AvRoomSize'] = properties_df['calculatedfinishedsquarefeet'] / properties_df['roomcnt']\n",
    "\n",
    "#Total number of rooms\n",
    "properties_df['N-TotalRooms'] = properties_df['calculatedbathnbr'] + properties_df['bedroomcnt']\n",
    "\n",
    "# Number of Extra rooms\n",
    "properties_df['N-ExtraRooms'] = properties_df['roomcnt'] - properties_df['N-TotalRooms']\n",
    "\n",
    "#Does property have a garage, pool or hot tub and AC?\n",
    "properties_df['N-GarPoolAC'] = ((properties_df['garagecarcnt']>0) & (properties_df['hashottuborspa']>0) & (properties_df['airconditioningtypeid']!=5))*1\n",
    "\n",
    "properties_df[\"N-location\"] = properties_df[\"latitude\"] + properties_df[\"longitude\"]\n",
    "properties_df[\"N-location-2\"] = properties_df[\"latitude\"] * properties_df[\"longitude\"]\n",
    "properties_df[\"N-location-2round\"] = properties_df[\"N-location-2\"].round(-4)\n",
    "\n",
    "properties_df[\"N-latitude-round\"] = properties_df[\"latitude\"].round(-4)\n",
    "properties_df[\"N-longitude-round\"] = properties_df[\"longitude\"].round(-4)\n",
    "\n",
    "# #Ratio of tax of property over parcel\n",
    "# properties_df['N-ValueRatio'] = properties_df['taxvaluedollarcnt'] / properties_df['taxamount']\n",
    "\n",
    "# #TotalTaxScore\n",
    "# properties_df['N-TaxScore'] = properties_df['taxvaluedollarcnt']*properties_df['taxamount']\n",
    "\n",
    "#polnomials of tax delinquency year\n",
    "properties_df[\"N-taxdelinquencyyear-2\"] = properties_df[\"taxdelinquencyyear\"] ** 2\n",
    "properties_df[\"N-taxdelinquencyyear-3\"] = properties_df[\"taxdelinquencyyear\"] ** 3\n",
    "\n",
    "#Length of time since unpaid taxes\n",
    "properties_df['N-life'] = 2018 - properties_df['taxdelinquencyyear']\n",
    "\n",
    "#Number of properties in the zip\n",
    "zip_count = properties_df['regionidzip'].value_counts().to_dict()\n",
    "properties_df['N-zip_count'] = properties_df['regionidzip'].map(zip_count)\n",
    "\n",
    "#Number of properties in the city\n",
    "city_count = properties_df['regionidcity'].value_counts().to_dict()\n",
    "properties_df['N-city_count'] = properties_df['regionidcity'].map(city_count)\n",
    "\n",
    "#Number of properties in the city\n",
    "region_count = properties_df['regionidcounty'].value_counts().to_dict()\n",
    "properties_df['N-county_count'] = properties_df['regionidcounty'].map(city_count)\n",
    "\n",
    "#Indicator whether it has AC or not\n",
    "properties_df['N-ACInd'] = (properties_df['airconditioningtypeid']!=5)*1\n",
    "\n",
    "#Indicator whether it has Heating or not \n",
    "properties_df['N-HeatInd'] = (properties_df['heatingorsystemtypeid']!=13)*1\n",
    "\n",
    "#polnomials of the variable\n",
    "properties_df[\"N-structuretaxvaluedollarcnt-2\"] = properties_df[\"structuretaxvaluedollarcnt\"] ** 2\n",
    "properties_df[\"N-structuretaxvaluedollarcnt-3\"] = properties_df[\"structuretaxvaluedollarcnt\"] ** 3\n",
    "\n",
    "#Average structuretaxvaluedollarcnt by city\n",
    "group = properties_df.groupby('regionidcity')['structuretaxvaluedollarcnt'].aggregate('mean').to_dict()\n",
    "properties_df['N-Avg-structuretaxvaluedollarcnt'] = properties_df['regionidcity'].map(group)\n",
    "\n",
    "#Deviation away from average\n",
    "properties_df['N-Dev-structuretaxvaluedollarcnt'] = abs((properties_df['structuretaxvaluedollarcnt'] - properties_df['N-Avg-structuretaxvaluedollarcnt']))/properties_df['N-Avg-structuretaxvaluedollarcnt']\n",
    "\n",
    "properties_df['hashottuborspa'] = properties_df['hashottuborspa'].astype(np.int32)\n",
    "properties_df['fireplaceflag'] = properties_df['fireplaceflag'].astype(np.int32)\n",
    "properties_df['taxdelinquencyflag'] = properties_df['taxdelinquencyflag'].astype(np.int32)\n",
    "properties_df['fireplaceflag'] = properties_df['fireplaceflag'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "properties_df['airconditioningtypeid'].fillna(value=0.0, inplace = True)\n",
    "properties_df['architecturalstyletypeid'].fillna(value=0.0, inplace = True)\n",
    "properties_df['basementsqft'].fillna(value=0.0, inplace = True)\n",
    "properties_df['bedroomcnt'].fillna(value=0.0, inplace = True)\n",
    "properties_df['buildingclasstypeid'].fillna(value=0.0, inplace = True)\n",
    "properties_df['buildingqualitytypeid'].fillna(value=0.0, inplace = True)\n",
    "\n",
    "properties_df['decktypeid'].fillna(value=0.0, inplace = True)\n",
    "properties_df['finishedfloor1squarefeet'].fillna(value=0.0, inplace = True)\n",
    "\n",
    "properties_df['finishedsquarefeet12'].fillna(value=0.0, inplace = True)\n",
    "properties_df['finishedsquarefeet13'].fillna(value=0.0, inplace = True)\n",
    "properties_df['finishedsquarefeet15'].fillna(value=0.0, inplace = True)\n",
    "properties_df['finishedsquarefeet6'].fillna(value=0.0, inplace = True)\n",
    "properties_df['fips'].fillna(value=0.0, inplace = True)\n",
    "properties_df['censustractandblock'].fillna(value = 0.0, inplace = True)\n",
    "properties_df['taxdelinquencyyear'].fillna(value = 0.0, inplace = True)\n",
    "properties_df['heatingorsystemtypeid'].fillna(value=0.0, inplace = True)\n",
    "properties_df['latitude'].fillna(value=0.0, inplace = True)\n",
    "properties_df['longitude'].fillna(value=0.0, inplace = True)\n",
    "properties_df['propertylandusetypeid'].fillna(value = 0.0, inplace = True)\n",
    "properties_df['rawcensustractandblock'].fillna(value = 0.0, inplace = True)\n",
    "\n",
    "properties_df['regionidcity'].fillna(value = 0.0, inplace = True)\n",
    "properties_df['regionidcounty'].fillna(value = 0.0, inplace = True)\n",
    "properties_df['regionidneighborhood'].fillna(value = 0.0, inplace = True)\n",
    "properties_df['regionidzip'].fillna(value = 0.0, inplace = True)\n",
    "properties_df['roomcnt'].fillna(value = 0.0, inplace = True)\n",
    "properties_df['yearbuilt'].fillna(value = 0.0, inplace = True)\n",
    "properties_df['structuretaxvaluedollarcnt'].fillna(value = 0.0, inplace = True)\n",
    "# properties_df['taxvaluedollarcnt'].fillna(value = 0.0, inplace = True)\n",
    "properties_df['assessmentyear'].fillna(value = 0.0, inplace = True)\n",
    "# properties_df['landtaxvaluedollarcnt'].fillna(value = 0.0, inplace = True)\n",
    "# properties_df['taxamount'].fillna(value = 0.0, inplace = True)\n",
    "properties_df['numberofstories'].fillna(value = 0.0, inplace = True)\n",
    "\n",
    "properties_df['poolsizesum'].fillna(value=0.0, inplace = True)\n",
    "properties_df['house_age'].fillna(value=1.0, inplace = True)\n",
    "properties_df['N-LivingAreaError'].fillna(value=0.0, inplace = True)\n",
    "properties_df['N-LivingAreaProp'].fillna(value=0.0, inplace = True)\n",
    "properties_df['N-ExtraSpace'].fillna(value=0.0, inplace = True)\n",
    "properties_df['N-TotalRooms'].fillna(value=0.0, inplace = True)\n",
    "properties_df['N-ExtraRooms'].fillna(value=0.0, inplace = True)\n",
    "# properties_df['N-ValueProp'].fillna(value=0.0, inplace = True)\n",
    "properties_df['N-location'].fillna(value=0.0, inplace = True)\n",
    "properties_df['N-location-2'].fillna(value=0.0, inplace = True)\n",
    "properties_df['N-location-2round'].fillna(value=0.0, inplace = True)\n",
    "properties_df['N-latitude-round'].fillna(value=0.0, inplace = True)\n",
    "properties_df['N-longitude-round'].fillna(value=0.0, inplace = True)\n",
    "# properties_df['N-ValueRatio'].fillna(value=0.0, inplace = True)\n",
    "# properties_df['N-TaxScore'].fillna(value=0.0, inplace = True)\n",
    "properties_df['N-taxdelinquencyyear-2'].fillna(value=0.0, inplace = True)\n",
    "properties_df['N-taxdelinquencyyear-3'].fillna(value=0.0, inplace = True)\n",
    "properties_df['N-life'].fillna(value=0.0, inplace = True)\n",
    "properties_df['N-zip_count'].fillna(value=0.0, inplace = True)\n",
    "properties_df['N-city_count'].fillna(value=0.0, inplace = True)\n",
    "properties_df['N-structuretaxvaluedollarcnt-2'].fillna(value=0.0, inplace = True)\n",
    "properties_df['N-structuretaxvaluedollarcnt-3'].fillna(value=0.0, inplace = True)\n",
    "properties_df['N-Avg-structuretaxvaluedollarcnt'].fillna(value=0.0, inplace = True)\n",
    "properties_df['N-Dev-structuretaxvaluedollarcnt'].fillna(value=0.0, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_df['storytypeid'].fillna(value=0.0, inplace = True)\n",
    "\n",
    "properties_df['threequarterbathnbr'].fillna(value=0.0, inplace = True)\n",
    "\n",
    "properties_df['typeconstructiontypeid'].fillna(value=0.0, inplace = True)\n",
    "\n",
    "properties_df['unitcnt'].fillna(value=1.0, inplace = True)\n",
    "\n",
    "properties_df['calculatedfinishedsquarefeet'].fillna(value=0.0,inplace = True)\n",
    "properties_df['yardbuildingsqft17'].fillna(value=0.0,inplace = True)\n",
    "properties_df['yardbuildingsqft26'].fillna(value=0.0,inplace = True)\n",
    "properties_df['calculatedbathnbr'].fillna(value=0.0, inplace = True)\n",
    "\n",
    "properties_df['fullbathcnt'].fillna(value=0.0, inplace = True)\n",
    "\n",
    "properties_df['garagetotalsqft'].fillna(value=0.0, inplace = True)\n",
    "\n",
    "properties_df['lotsizesquarefeet'].fillna(value=0.0, inplace = True)\n",
    "properties_df.drop(['N-county_count', 'N-ExtraSpace-2', 'N-LivingAreaProp2'], inplace = True, axis = 1)\n",
    "#because this column contains so much inf values\n",
    "properties_df.drop(['N-AvRoomSize'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "properties_df.drop_duplicates(subset = ['parcelid'], inplace = True, keep = 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for c, dtype in zip(properties_df.columns, properties_df.dtypes):\n",
    "    if (dtype == np.float64) | (dtype == np.int):\n",
    "        properties_df[c] = properties_df[c].astype(np.float32) \n",
    "#     elif dtype == np.int64:\n",
    "#         properties_df[c] = properties_df[c].astype(np.int32)\n",
    "#end for loop\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_train = properties_df[properties_df['logerror'].notnull()].drop(['logerror', 'parcelid', 'propertyzoningdesc', 'propertycountylandusecode'], axis = 1)\n",
    "\n",
    "exog_test = properties_df[properties_df['logerror'].isnull()]\n",
    "\n",
    "parcelids_test = exog_test['parcelid']\n",
    "\n",
    "exog_test.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode'], axis = 1, inplace = True)\n",
    "\n",
    "timeseries_train = properties_df[properties_df['logerror'].notnull()][['transactiondate', 'logerror']]\n",
    "\n",
    "timeseries_test = properties_df[properties_df['logerror'].isnull()][['transactiondate', 'logerror']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_train.index = exog_train['transactiondate']\n",
    "exog_train.drop(['transactiondate'], axis = 1, inplace = True)\n",
    "\n",
    "timeseries_train.index = timeseries_train['transactiondate']\n",
    "timeseries_train.drop(['transactiondate'], axis = 1, inplace = True)\n",
    "\n",
    "timeseries_test.index = timeseries_test['transactiondate']\n",
    "timeseries_test.drop(['transactiondate'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timeseries_train_grouped = timeseries_train.groupby('transactiondate').mean()\n",
    "exog_train_grouped = exog_train.groupby('transactiondate').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exog_train_grouped = exog_train_grouped.asfreq('d', method = 'bfill')\n",
    "timeseries_train_grouped = timeseries_train_grouped.asfreq('d', method = 'bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exog_train_grouped.loc[ : pd.to_datetime('2016-09-30')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_exog = scaler.fit(exog_train_grouped)\n",
    "scaler_endog = scaler.fit(timeseries_train_grouped)\n",
    "\n",
    "#transformed_exog_train = scaler_exog.transform(exog_train_grouped)\n",
    "#transformed_endog_train = scaler_endog.transform(timeseries_train_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "endog_sep16 = timeseries_train_grouped.loc[ : pd.to_datetime('2016-09-30')]\n",
    "endog_oct16 = timeseries_train_grouped.loc[ : pd.to_datetime('2016-10-31')]\n",
    "endog_nov16 = timeseries_train_grouped.loc[ : pd.to_datetime('2016-11-30')]\n",
    "# endog_17 = timeseries_train_grouped.iloc[ : len(timeseries_train_grouped)]\n",
    "\n",
    "exog_sep16 = exog_train_grouped.loc[ : pd.to_datetime('2016-09-30')]\n",
    "exog_oct16 = exog_train_grouped.loc[ : pd.to_datetime('2016-10-31')]\n",
    "exog_nov16 = exog_train_grouped.loc[ : pd.to_datetime('2016-11-30')]\n",
    "# exog_17 = exog_train_grouped.iloc[ : len(exog_train_grouped)]\n",
    "\n",
    "\n",
    "# make record of start and end dates of endog before reshaping (after reshaping we will lose time index)\n",
    "endog_sep16_start = endog_sep16.iloc[0].name\n",
    "endog_sep16_end = endog_sep16.iloc[len(endog_sep16) - 1].name\n",
    "\n",
    "endog_oct16_start = endog_oct16.iloc[0].name\n",
    "endog_oct16_end = endog_oct16.iloc[len(endog_oct16) - 1].name\n",
    "\n",
    "endog_nov16_start = endog_nov16.iloc[0].name\n",
    "endog_nov16_end = endog_nov16.iloc[len(endog_nov16) - 1].name\n",
    "\n",
    "# endog_17_start = endog_17.iloc[0].name\n",
    "# endog_17_end = endog_17.iloc[len(endog_17) - 1].name\n",
    "\n",
    "\n",
    "# transform the data to MinMaxScaler\n",
    "endog_sep16 = scaler_endog.transform(endog_sep16)\n",
    "endog_oct16 = scaler_endog.transform(endog_oct16)\n",
    "endog_nov16 = scaler_endog.transform(endog_nov16)\n",
    "# endog_17 = scaler_endog.transfrom(endog_17)\n",
    "\n",
    "exog_sep16 = scaler_exog.transform(exog_sep16)\n",
    "exog_oct16 = scaler_exog.transform(exog_oct16)\n",
    "exog_nov16 = scaler_exog.transform(exog_nov16)\n",
    "# exog_17 = scaler_exog.transform(exog_17)\n",
    "\n",
    "exog_test = scaler_exog.transform(exog_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exog_sep16 = exog_sep16[:6,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog2_sep16 = np.array([])\n",
    "exog2_sep16 = np.concatenate([exog_sep16[x : x + 30,:].reshape(1, 30, exog_sep16.shape[1]) for x in range(exog_sep16.shape[0]-29)])\n",
    "\n",
    "endog_sep16 = endog_sep16.reshape(endog_sep16.shape[0],)\n",
    "endog_sep16 = endog_sep16[ 29 : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((245, 30, 72), (245,))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exog2_sep16.shape, endog_sep16.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples = 60 # how far back to look\n",
    "# y_examples = 92 # how many steps forward to predict\n",
    "# nb_samples = len(exog_train_grouped) - examples - y_examples\n",
    "\n",
    "# input_list = [np.expand_dims(np.atleast_2d(exog_sep16[i:examples+i,:]), axis=0) for i in range(nb_samples)]\n",
    "# exog2_sep16 = np.concatenate(input_list, axis=0)\n",
    "\n",
    "# test_input_list = [np.expand_dims(np.atleast_2d(exog_test[len(exog_test)-examples:len(exog_test),:]), axis=0) for i in range(1)]\n",
    "# exog2_test = np.concatenate(test_input_list, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((245, 30, 72), (2895037, 72))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exog2_sep16.shape, exog_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# reshape enxog variables according to the specifications of keras lstm\n",
    "\n",
    "# exog_sep16 = exog_sep16.reshape(exog_sep16.shape[0], 1, exog_sep16.shape[1])\n",
    "# exog_oct16 = exog_oct16.reshape(exog_oct16.shape[0], 1, exog_oct16.shape[1])\n",
    "# exog_nov16 = exog_nov16.reshape(exog_nov16.shape[0], 1, exog_nov16.shape[1])\n",
    "# # exog_17 = exog_17.reshape(exog_17.shape[0], 1, exog_17.shape[1])\n",
    "\n",
    "# endog_sep16 = endog_sep16.reshape(endog_sep16.shape[0],)\n",
    "# endog_oct16 = endog_oct16.reshape(endog_oct16.shape[0],)\n",
    "# endog_nov16 = endog_nov16.reshape(endog_nov16.shape[0],)\n",
    "# # endog_17 = endog_17.reshape(endog_17.shape[0], )\n",
    "\n",
    "exog_test = exog_test.reshape(exog_test.shape[0], 1, exog_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#endog_exog_tuples = [(endog_sep16, exog_sep16), (endog_oct16, exog_oct16), (endog_nov16, exog_nov16)]\n",
    "endog_exog_tuples = [(endog_sep16, exog2_sep16, endog_sep16_start, endog_sep16_end)\n",
    "#                     (endog_oct16, exog_oct16, endog_oct16_start, endog_oct16_end),\n",
    "#                     (endog_nov16, exog_nov16, endog_nov16_start, endog_nov16_end)\n",
    "#                     (endog_17, exog_17, endog_17_start, endog_17_end)\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = {'201610' : pd.DataFrame(columns = ['201610'], dtype = np.float32),\n",
    "#                 '201611' : pd.DataFrame(columns = ['201611'], dtype = np.float32),\n",
    "#                 '201612' : pd.DataFrame(columns = ['201612'], dtype = np.float32),\n",
    "#                 '2017' : pd.DataFrame(columns = ['2017'], dtype = np.float32),\n",
    "#                 #'201711' : pd.DataFrame(columns = ['201711'], dtype = np.float32),\n",
    "#                 #'201712' : pd.DataFrame(columns = ['201712'], dtype = np.float32)\n",
    "#                }\n",
    "\n",
    "\n",
    "# def get_predictions(rows, prediction_column):\n",
    "#     global predictions\n",
    "    \n",
    "#     #prediction = sarimax_result.get_prediction(start = start, end = end, dynamic = True, full_results = True, exog = rows)\n",
    "    \n",
    "#     model = load_model('model_' + prediction_column + '.h5')\n",
    "#     # make predictions\n",
    "#     prediction = model.predict(rows, verbose=0)\n",
    "#     print(prediction.shape)\n",
    "    \n",
    "#     df = pd.DataFrame(prediction[len(prediction) - 1], columns = [prediction_column])\n",
    "    \n",
    "#     predictions[prediction_column] = predictions[prediction_column].append(df)\n",
    "\n",
    "# #get_predictions end\n",
    "\n",
    "\n",
    "# for endog, exog, endog_start, endog_end in endog_exog_tuples:\n",
    "# # def sarimax_model_func(endog, exog):\n",
    "# #     sarimax_model = SARIMAX(endog = endog,\n",
    "# #                                     order = (0, 0, 0),\n",
    "# #                                     exog = exog,\n",
    "# #                                     seasonal_order=(1, 0, 0, 12),\n",
    "# #                                     enforce_stationarity=False,\n",
    "# #                                     enforce_invertibility=False)\n",
    "\n",
    "# #     sarimax_result = sarimax_model.fit()\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(100, return_sequences=True, input_shape = (exog.shape[1], exog.shape[2])))\n",
    "#     model.add(LSTM(100))\n",
    "#     model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "#     model.compile(loss='mse', optimizer='adam')\n",
    "    \n",
    "#     model.fit(exog, endog, epochs=500, shuffle=False, verbose=1)\n",
    "    \n",
    "#     start_date = endog_end + timedelta(days = 1)\n",
    "#     end_date = endog_end + timedelta(days = 15)\n",
    "    \n",
    "#     delta = end_date - start_date\n",
    "    \n",
    "#     year_of_prediction = end_date.year\n",
    "#     month_of_prediction = end_date.month\n",
    "\n",
    "#     prediction_column = str(year_of_prediction) + str(month_of_prediction)\n",
    "    \n",
    "#     model.save('model_' + prediction_column + '.h5')\n",
    "    \n",
    "#     if (str(year_of_prediction) == '2017'):\n",
    "#         prediction_column = str(year_of_prediction)\n",
    "    \n",
    "    \n",
    "#     for x in range(0, len(exog_test)):\n",
    "#         record = np.array([exog_test[x]])\n",
    "        \n",
    "# #         record = record.append([record] * (delta.days - 1))\n",
    "# #         record.index = range(0, delta.days)\n",
    "#         if (prediction_column != '2017'):\n",
    "#             record = np.repeat(record, delta.days + 1, axis = 0)\n",
    "            \n",
    "#         else:\n",
    "#             record = np.repeat(record, 92, axis = 0)      #92 is number of days of October + November + December\n",
    "    \n",
    "#         record = record.reshape(record.shape[0], 1, record.shape[1])\n",
    "\n",
    "# #         datelist = pd.date_range(start_date, periods = delta.days, freq = 'D')\n",
    "# #         transactiondate = pd.DataFrame(datelist, columns = ['transactiondate'])\n",
    "\n",
    "# #         record.index = datelist\n",
    "# #         record.index.name = 'transactiondate'\n",
    "        \n",
    "#         prediction = get_predictions(record, prediction_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "245/245 [==============================] - 4s - loss: 0.0525     \n",
      "Epoch 2/50\n",
      "245/245 [==============================] - 0s - loss: 0.0349     \n",
      "Epoch 3/50\n",
      "245/245 [==============================] - 0s - loss: 0.0375     \n",
      "Epoch 4/50\n",
      "245/245 [==============================] - 0s - loss: 0.0249     \n",
      "Epoch 5/50\n",
      "245/245 [==============================] - 0s - loss: 0.0251     \n",
      "Epoch 6/50\n",
      "245/245 [==============================] - 0s - loss: 0.0216     \n",
      "Epoch 7/50\n",
      "245/245 [==============================] - 0s - loss: 0.0224     \n",
      "Epoch 8/50\n",
      "245/245 [==============================] - 0s - loss: 0.0215     \n",
      "Epoch 9/50\n",
      "245/245 [==============================] - 0s - loss: 0.0210     \n",
      "Epoch 10/50\n",
      "245/245 [==============================] - 0s - loss: 0.0212     \n",
      "Epoch 11/50\n",
      "245/245 [==============================] - 0s - loss: 0.0207     \n",
      "Epoch 12/50\n",
      "245/245 [==============================] - 0s - loss: 0.0205     \n",
      "Epoch 13/50\n",
      "245/245 [==============================] - 0s - loss: 0.0204     \n",
      "Epoch 14/50\n",
      "245/245 [==============================] - 0s - loss: 0.0202     \n",
      "Epoch 15/50\n",
      "245/245 [==============================] - 0s - loss: 0.0200     \n",
      "Epoch 16/50\n",
      "245/245 [==============================] - 0s - loss: 0.0199     \n",
      "Epoch 17/50\n",
      "245/245 [==============================] - 0s - loss: 0.0197     \n",
      "Epoch 18/50\n",
      "245/245 [==============================] - 0s - loss: 0.0195     \n",
      "Epoch 19/50\n",
      "245/245 [==============================] - 0s - loss: 0.0193     \n",
      "Epoch 20/50\n",
      "245/245 [==============================] - 0s - loss: 0.0191     \n",
      "Epoch 21/50\n",
      "245/245 [==============================] - 0s - loss: 0.0190     \n",
      "Epoch 22/50\n",
      "245/245 [==============================] - 0s - loss: 0.0187     \n",
      "Epoch 23/50\n",
      "245/245 [==============================] - 0s - loss: 0.0184     \n",
      "Epoch 24/50\n",
      "245/245 [==============================] - 0s - loss: 0.0181     \n",
      "Epoch 25/50\n",
      "245/245 [==============================] - 0s - loss: 0.0179     \n",
      "Epoch 26/50\n",
      "245/245 [==============================] - 0s - loss: 0.0176     \n",
      "Epoch 27/50\n",
      "245/245 [==============================] - 0s - loss: 0.0172     \n",
      "Epoch 28/50\n",
      "245/245 [==============================] - 0s - loss: 0.0167     \n",
      "Epoch 29/50\n",
      "245/245 [==============================] - 0s - loss: 0.0160     \n",
      "Epoch 30/50\n",
      "245/245 [==============================] - 0s - loss: 0.0154     \n",
      "Epoch 31/50\n",
      "245/245 [==============================] - 0s - loss: 0.0147     \n",
      "Epoch 32/50\n",
      "245/245 [==============================] - 0s - loss: 0.0139     \n",
      "Epoch 33/50\n",
      "245/245 [==============================] - 0s - loss: 0.0135     \n",
      "Epoch 34/50\n",
      "245/245 [==============================] - 0s - loss: 0.0147     \n",
      "Epoch 35/50\n",
      "245/245 [==============================] - 0s - loss: 0.0161     \n",
      "Epoch 36/50\n",
      "245/245 [==============================] - 0s - loss: 0.0159     \n",
      "Epoch 37/50\n",
      "245/245 [==============================] - 0s - loss: 0.0167     \n",
      "Epoch 38/50\n",
      "245/245 [==============================] - 0s - loss: 0.0165     \n",
      "Epoch 39/50\n",
      "245/245 [==============================] - 0s - loss: 0.0152     \n",
      "Epoch 40/50\n",
      "245/245 [==============================] - 0s - loss: 0.0140     \n",
      "Epoch 41/50\n",
      "245/245 [==============================] - 0s - loss: 0.0141     \n",
      "Epoch 42/50\n",
      "245/245 [==============================] - 0s - loss: 0.0124     \n",
      "Epoch 43/50\n",
      "245/245 [==============================] - 0s - loss: 0.0117     \n",
      "Epoch 44/50\n",
      "245/245 [==============================] - 0s - loss: 0.0127     \n",
      "Epoch 45/50\n",
      "245/245 [==============================] - 0s - loss: 0.0123     \n",
      "Epoch 46/50\n",
      "245/245 [==============================] - 0s - loss: 0.0123     \n",
      "Epoch 47/50\n",
      "245/245 [==============================] - 0s - loss: 0.0112     \n",
      "Epoch 48/50\n",
      "245/245 [==============================] - 0s - loss: 0.0127     \n",
      "Epoch 49/50\n",
      "245/245 [==============================] - 0s - loss: 0.0121     \n",
      "Epoch 50/50\n",
      "245/245 [==============================] - 0s - loss: 0.0129     \n"
     ]
    }
   ],
   "source": [
    "predictions = {'201610' : pd.DataFrame(columns = ['201610'], dtype = np.float32),\n",
    "                '201611' : pd.DataFrame(columns = ['201611'], dtype = np.float32),\n",
    "                '201612' : pd.DataFrame(columns = ['201612'], dtype = np.float32),\n",
    "                '2017' : pd.DataFrame(columns = ['2017'], dtype = np.float32),\n",
    "                #'201711' : pd.DataFrame(columns = ['201711'], dtype = np.float32),\n",
    "                #'201712' : pd.DataFrame(columns = ['201712'], dtype = np.float32)\n",
    "               }\n",
    "\n",
    "\n",
    "def get_predictions(rows, prediction_column):\n",
    "    global predictions\n",
    "    \n",
    "    #prediction = sarimax_result.get_prediction(start = start, end = end, dynamic = True, full_results = True, exog = rows)\n",
    "    \n",
    "    model = load_model('model_' + prediction_column + '.h5')\n",
    "    # make predictions\n",
    "    prediction = model.predict(rows, verbose=0)\n",
    "    print(prediction.shape)\n",
    "    \n",
    "    df = pd.DataFrame(prediction[len(prediction) - 1], columns = [prediction_column])\n",
    "    \n",
    "    predictions[prediction_column] = predictions[prediction_column].append(df)\n",
    "\n",
    "#get_predictions end\n",
    "\n",
    "\n",
    "for endog, exog, endog_start, endog_end in endog_exog_tuples:\n",
    "# def sarimax_model_func(endog, exog):\n",
    "#     sarimax_model = SARIMAX(endog = endog,\n",
    "#                                     order = (0, 0, 0),\n",
    "#                                     exog = exog,\n",
    "#                                     seasonal_order=(1, 0, 0, 12),\n",
    "#                                     enforce_stationarity=False,\n",
    "#                                     enforce_invertibility=False)\n",
    "\n",
    "#     sarimax_result = sarimax_model.fit()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=True, input_shape = (exog.shape[1], exog.shape[2])))\n",
    "    model.add(LSTM(100, return_sequences = False))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    \n",
    "    model.fit(exog, endog, epochs=50, shuffle=False, verbose=1)\n",
    "    \n",
    "    start_date = endog_end + timedelta(days = 1)\n",
    "    end_date = endog_end + timedelta(days = 15)\n",
    "    \n",
    "    delta = end_date - start_date\n",
    "    \n",
    "    year_of_prediction = end_date.year\n",
    "    month_of_prediction = end_date.month\n",
    "\n",
    "    prediction_column = str(year_of_prediction) + str(month_of_prediction)\n",
    "    \n",
    "    model.save('model_lstm.h5')\n",
    "    \n",
    "    if (str(year_of_prediction) == '2017'):\n",
    "        prediction_column = str(year_of_prediction)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected lstm_3_input to have shape (None, 30, 72) but got array with shape (2895067, 1, 72)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-fdc7db73146f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model_lstm.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexog_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#     for x in range(0, len(exog_test)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\AnacondaIDE\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m    911\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 913\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\AnacondaIDE\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1693\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[0;32m   1694\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m                                     check_batch_axis=False)\n\u001b[0m\u001b[0;32m   1696\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\AnacondaIDE\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    142\u001b[0m                             \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                             \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                             str(array.shape))\n\u001b[0m\u001b[0;32m    145\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking : expected lstm_3_input to have shape (None, 30, 72) but got array with shape (2895067, 1, 72)"
     ]
    }
   ],
   "source": [
    "model = load_model('model_lstm.h5')\n",
    "\n",
    "prediction = model.predict(exog_test, verbose=1)\n",
    "#     for x in range(0, len(exog_test)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 30, 72)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-868-34f5ed9db67c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'prediction' is not defined"
     ]
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.7238661 , -0.43128777, -0.31542821, -0.35185375, -0.26344065,\n",
       "       -0.25590285, -0.34791175, -0.28571553, -0.13263179, -0.31547805,\n",
       "       -0.26223067, -0.34433794, -0.15443062, -0.22619402, -0.20622291,\n",
       "        0.13502814, -0.20742747, -0.36874538, -0.30429553, -0.20858801,\n",
       "       -0.32533195, -0.21030938, -0.24567617, -0.34174079, -0.09912329,\n",
       "       -0.11859562, -0.24164009, -0.20844921, -0.30892482, -0.30778981,\n",
       "       -0.37169636, -0.22513671, -0.2436683 , -0.1569894 , -0.09575444,\n",
       "       -0.21681482, -0.396791  , -0.17618252, -0.29318558, -0.17295309,\n",
       "       -0.18015012, -0.30362712, -0.22839623, -0.3404876 , -0.75625998,\n",
       "       -0.55692539, -0.32454138, -0.21400226, -0.21648424, -0.24459758,\n",
       "       -0.22136556, -0.41754835, -0.2125102 , -0.34707138, -0.30005142,\n",
       "       -0.28744029, -0.22827616, -0.42624689, -0.47433832, -0.29534625,\n",
       "       -0.27684467, -0.33520319, -0.24394325, -0.18392483, -0.51092024,\n",
       "       -0.40264436, -0.23071884, -0.33889317, -0.29504415, -0.2886909 ,\n",
       "       -0.32175058, -0.48963325, -0.48963325, -0.25283319, -0.3052546 ,\n",
       "       -0.25969717, -0.1355784 , -0.3846239 , -0.53545877, -0.39489476,\n",
       "       -0.24061635, -0.27512832, -0.38998388, -0.22783678, -0.39811298,\n",
       "       -0.2992811 , -0.2992811 , -0.29765572, -0.31438356, -0.30943351,\n",
       "       -0.35112493, -0.2661251 ,  0.10512611, -0.73431965, -0.31764578,\n",
       "       -0.13549108, -0.19710918, -0.45505692, -0.2676744 , -0.38410085,\n",
       "       -0.55891853, -0.43467589, -0.3479042 , -0.30859052, -0.38260028,\n",
       "       -0.33069455, -0.45280267, -0.46636799, -0.43690492, -0.39331761,\n",
       "       -0.2489222 , -0.34635096, -0.32268216, -0.3372786 , -0.47282578,\n",
       "       -0.2329274 , -0.22550797, -0.36082955, -0.37351166, -0.37025772,\n",
       "       -0.25540044, -0.24701327, -0.25739193, -0.40019448, -0.27310498,\n",
       "       -0.26803776, -0.26654987, -0.39324164, -0.32371327, -0.28357436,\n",
       "       -0.31284358, -0.39057562, -0.27448591, -0.41777874, -0.41546155,\n",
       "       -0.32188997, -0.31369838, -0.37424372, -0.3078263 , -0.42576656,\n",
       "       -0.29189927, -0.42016565, -0.42016565, -0.36616833, -0.35478813,\n",
       "       -0.28104738, -0.29056447, -0.29158437, -0.06164477, -0.30971035,\n",
       "       -0.36771549, -0.30756369, -0.35060139, -0.25143082, -0.31865657,\n",
       "       -0.80980413, -0.36630548, -0.34842907, -0.26612641, -0.36343515,\n",
       "       -0.43908341, -0.38455944,  1.        , -0.45744379, -0.3354343 ,\n",
       "       -0.32125427, -0.30278015, -0.25881314, -0.27156815, -0.46544419,\n",
       "       -0.21613876, -0.35783153, -0.330139  , -0.32938838, -0.31194201,\n",
       "       -0.29390566, -0.23655968, -0.26128359, -0.28974346, -0.26467011,\n",
       "       -0.3320475 , -0.31930064, -0.24983999, -0.24640549, -0.88164201,\n",
       "       -0.88164201, -0.13650522, -0.29279122, -0.27241001, -0.34806221,\n",
       "       -0.03964366, -0.21650342, -0.31476407, -0.23429596, -0.3173095 ,\n",
       "       -0.26675426, -0.33435795, -0.27537933, -0.46139241, -0.29590851,\n",
       "       -0.20463428, -0.35345905, -0.3058107 , -0.3629524 , -0.25340696,\n",
       "       -0.39044593, -0.36156255, -0.31270905, -0.22516241, -0.23243094,\n",
       "       -0.24381361, -0.60206989, -1.        , -0.25228411, -0.24049921,\n",
       "       -0.27111091, -0.24458121, -0.35617453, -0.11877474, -0.26573243,\n",
       "       -0.22919271, -0.24832396, -0.36365539, -0.40407608, -0.33537377,\n",
       "       -0.2347364 , -0.32541502, -0.33936714, -0.33616935, -0.24958959,\n",
       "       -0.20981198, -0.31392504, -0.2347364 , -0.18587205, -0.31120413,\n",
       "       -0.25149365, -0.26624093, -0.35525796, -0.36070729, -0.31233589,\n",
       "       -0.2793707 , -0.23984745, -0.22045819, -0.18752266, -0.19346285,\n",
       "       -0.28873509, -0.33416684, -0.4143311 , -0.16885461, -0.15601772,\n",
       "       -0.24531599, -0.25386483, -0.24214159, -0.27689102, -0.22058762,\n",
       "       -0.32562272, -0.1773621 , -0.28098024, -0.24506393, -0.2584882 ,\n",
       "       -0.38297041, -0.29133152, -0.28177505, -0.26507571, -0.24794474,\n",
       "       -0.22758908, -0.21736795, -0.40539695, -0.10953669, -0.24201616,\n",
       "       -0.28319519, -0.28740503, -0.24209411, -0.3202021 ])"
      ]
     },
     "execution_count": 1205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endog_sep16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1206,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  4.49027601e+00,  -3.71848297e-01,  -3.71848297e-01, ...,\n",
       "           1.06175239e+17,   1.24734085e+06,   5.16668131e+00]],\n",
       "\n",
       "       [[  2.05921386e+00,  -3.71848297e-01,  -3.71848297e-01, ...,\n",
       "           8.44426594e+17,   1.39098642e+06,   4.01803817e+00]],\n",
       "\n",
       "       [[  9.25515792e+00,  -3.71848297e-01,  -3.71848297e-01, ...,\n",
       "           4.08800518e+17,   1.24441013e+06,   3.69129399e+00]],\n",
       "\n",
       "       ..., \n",
       "       [[  3.80037999e+00,  -3.71848297e-01,  -3.71848297e-01, ...,\n",
       "           1.35414902e+18,   1.23823291e+06,   3.54601899e+00]],\n",
       "\n",
       "       [[  3.73994822e+00,  -3.71848297e-01,   8.40698741e+00, ...,\n",
       "           3.93863169e+17,   1.25693299e+06,   3.29993440e+00]],\n",
       "\n",
       "       [[  3.56843035e+00,  -3.71848297e-01,   1.53710656e+00, ...,\n",
       "           3.28306336e+17,   1.22667253e+06,   3.36569960e+00]]])"
      ]
     },
     "execution_count": 1206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exog_sep16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ -3.71848297e-01,  -3.71848297e-01,  -3.71848297e-01, ...,\n",
       "          -3.71848297e-01,   2.73982042e+06,  -3.71848297e-01]],\n",
       "\n",
       "       [[ -3.71848297e-01,  -3.71848297e-01,  -3.71848297e-01, ...,\n",
       "          -3.71848297e-01,   2.73982042e+06,  -3.71848297e-01]],\n",
       "\n",
       "       [[ -3.71848297e-01,  -3.71848297e-01,  -3.71848297e-01, ...,\n",
       "           2.00988794e+18,   1.05992636e+06,   2.49920150e+01]],\n",
       "\n",
       "       ..., \n",
       "       [[ -3.71848297e-01,  -3.71848297e-01,  -3.71848297e-01, ...,\n",
       "          -3.71848297e-01,  -3.71848297e-01,  -3.71848297e-01]],\n",
       "\n",
       "       [[ -3.71848297e-01,  -3.71848297e-01,  -3.71848297e-01, ...,\n",
       "          -3.71848297e-01,  -3.71848297e-01,  -3.71848297e-01]],\n",
       "\n",
       "       [[ -3.71848297e-01,  -3.71848297e-01,  -3.71848297e-01, ...,\n",
       "          -3.71848297e-01,  -3.71848297e-01,  -3.71848297e-01]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exog_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 274, 72)"
      ]
     },
     "execution_count": 947,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exog_sep16.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2895067, 1, 72)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exog_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 274)"
      ]
     },
     "execution_count": 946,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endog_sep16.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 917,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exog_sep16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
